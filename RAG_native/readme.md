# RAG1 â€“ Native Python RAG Pipeline (No LangChain)

This project demonstrates a simple Retrieval-Augmented Generation (RAG) pipeline using **native Python**, **FAISS**, and **local LLMs** via **Ollama**.

---

## ğŸ”§ Tech Stack
- ğŸ§¾ Markdown file loader (`os`, `glob`)
- âœ‚ï¸ Text splitting (`nltk`)
- ğŸ” Vector search (`FAISS`)
- ğŸ§  Embeddings: `all-MiniLM-L6-v2` via `sentence-transformers`
- ğŸ’¬ LLM: `llama2` (local, via Ollama API)

---

## ğŸ” Workflow
1. Loads markdown files (e.g. `bio.md`) from the `docs/` folder.
2. Splits text into overlapping chunks.
3. Converts chunks into vector embeddings.
4. Stores and queries embeddings with FAISS.
5. Sends top results + query to a local LLM (`llama2` via Ollama).
6. Returns a generated answer based on the documents.

---

## ğŸš€ How to Run

1. **Install dependencies**
    ```bash
    cd RAG1
    pip install -r requirements.txt
    ```

2. **Start the Ollama server**  
   Make sure Ollama is running and the `llama2` model is available:
    ```bash
    ollama run llama2
    ```

3. **Run the pipeline**
    ```bash
    python main.py
    ```

---

## ğŸ“ Project Structure

- `docs/` â€” Markdown files with dummy data
- `embed.py` â€” Embedding and FAISS logic
- `answer.py` â€” LLM generation via Ollama API
- `main.py` â€” Main RAG pipeline

---

## âœ… Pros

- Minimal dependencies
- Transparent and flexible
- Good for learning the mechanics of RAG

---

## âš ï¸ Limitations

- Manual pipeline management
- No abstraction layers
- No automatic document loading


# RAG1_langchain â€“ RAG with LangChain + Ollama

This project implements a more modular and scalable **RAG pipeline using LangChain**, integrated with **local Ollama models** (`llama2`, `tinyllama`).

---

## ğŸ”§ Tech Stack
- ğŸ“„ Document loader: `DirectoryLoader`
- âœ‚ï¸ Text splitter: `RecursiveCharacterTextSplitter`
- ğŸ§  Embeddings: `HuggingFaceEmbeddings`
- ğŸ” Vector store: `FAISS`
- ğŸ’¬ LLM: `llama2` via `Ollama`
- ğŸ§± RAG Chain: `RetrievalQA`

---

## ğŸ” Workflow
1. Loads markdown documents from `docs/`
2. Splits them into chunks for embedding
3. Converts chunks into vector embeddings
4. Stores and queries them using FAISS
5. Retrieves relevant docs using semantic similarity
6. Uses a local LLM to generate an answer with context

---
## ğŸš€ How to Run

1. **Clone and enter the project directory:**
    ```bash
    cd RAG1_langchain
    ```
2. **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
3. **Start Ollama (ensure it's running):**
    ```bash
    ollama run llama2
    ```
4. **Run the main pipeline:**
    ```bash
    python main.py
    ```

---

## ğŸ“ Project Structure

- `main.py` â€“ Entire LangChain RAG pipeline
- `docs/` â€“ Dummy markdown documents

---

## âš ï¸ Notes on Warnings

- Some components (Ollama, HuggingFaceEmbeddings, FAISS) are deprecated and will be removed in LangChain v1.0.
- **Update imports to:**
  - `langchain_community.vectorstores`
  - `langchain_huggingface.embeddings`
  - `langchain_ollama.llms`

---

## âœ… Pros

- Cleaner abstraction
- Easy to swap LLMs, retrievers, splitters
- Supports advanced LangChain features (tracing, agents, tools)

---

## âš ï¸ Limitations

- Slightly more overhead
- Deprecation warnings in current version

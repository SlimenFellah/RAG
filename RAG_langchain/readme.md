# ğŸ” RAG_langchain â€“ Modular RAG Pipeline with LangChain + Ollama

A production-ready, modular Retrieval-Augmented Generation (RAG) system built using **LangChain**, integrated with **local Ollama models** (`llama2`, `tinyllama`). This version supports embedding-based semantic search, flexible caching, and custom data pipelines.

---

## ğŸ§  What is RAG?

**Retrieval-Augmented Generation (RAG)** is a method that improves the factual accuracy of language models by augmenting their input with relevant documents retrieved from a knowledge base. This project implements RAG using:

- ğŸ“ Local Markdown/Text files as the knowledge base
- ğŸ” FAISS vector store for fast semantic search
- ğŸ¤– Ollama for running local LLMs like `llama2`

---

## ğŸ—ï¸ Modular Tech Stack

| Component            | Implementation                             |
|----------------------|---------------------------------------------|
| ğŸ“„ Document Loader   | `DirectoryLoader` via `data_loader.py`      |
| âœ‚ï¸ Text Splitter     | `RecursiveCharacterTextSplitter`            |
| ğŸ§  Embeddings        | `HuggingFaceEmbeddings` (local)             |
| ğŸ’¾ Vector Store      | `FAISS` stored locally                      |
| ğŸ’¬ LLM               | `llama2` via `Ollama`                       |
| ğŸ”— RAG Pipeline      | `RetrievalQA` in `rag_pipeline.py`          |
| ğŸ§¹ Cache Management  | `cache.py`, `cache_cleaner.py`, `file_tracker.py` |

---

## ğŸ” Workflow Overview

```mermaid
graph TD
    A[ğŸ“ Load Docs] --> B[âœ‚ï¸ Split Text]
    B --> C[ğŸ§  Generate Embeddings]
    C --> D[ğŸ’¾ Store in FAISS Vector DB]
    D --> E[ğŸ” Retrieve Relevant Chunks]
    E --> F[ğŸ¤– LLM with Context]
    F --> G[ğŸ“ Final Answer]
```

---

## ğŸš€ Getting Started

1. **Install Dependencies**
    ```bash
    pip install -r requirements.txt
    ```

2. **Start Ollama (LLM must be downloaded)**
    ```bash
    ollama run llama2
    ```
    You can also try tinyllama for faster responses:
    ```bash
    ollama run tinyllama
    ```

3. **Run the Main Pipeline**
    ```bash
    python main.py
    ```

---

## ğŸ“ Directory Structure

```
RAG_langchain/
â”‚
â”œâ”€â”€ main.py                 # Entry point script
â”œâ”€â”€ config.py               # Central configuration for paths, model names, etc.
â”œâ”€â”€ data_loader.py          # Loads and splits documents
â”œâ”€â”€ embedder.py             # Embedding generation using HuggingFace
â”œâ”€â”€ vectorstore.py          # Handles FAISS DB creation & queries
â”œâ”€â”€ rag_pipeline.py         # Assembles the RetrievalQA chain
â”œâ”€â”€ cache.py                # Query/result caching
â”œâ”€â”€ cache_cleaner.py        # Optional tool to clean cache
â”œâ”€â”€ file_tracker.py         # Tracks processed files
â”œâ”€â”€ query_cache.json        # Stores cached responses
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ readme.md               # You're here!
```

---

## ğŸ”„ Caching System

To avoid redundant processing and speed up dev cycles:

- âœ… Embedding cache
- âœ… Query result cache (`query_cache.json`)
- âœ… File change detection (`file_tracker.py`)

To clear the cache manually:

```bash
python cache_cleaner.py
```

---

## âš ï¸ Notes on LangChain Deprecations

LangChain v1.0+ deprecates many older modules. Update your imports as follows:

| Old Module                    | New Module                                 |
|-------------------------------|--------------------------------------------|
| `langchain.vectorstores`      | `langchain_community.vectorstores`         |
| `langchain.embeddings`        | `langchain_huggingface.embeddings`         |
| `langchain.llms`              | `langchain_ollama.llms`                    |

---

## âœ… Benefits

- ğŸ”§ Modular architecture â€“ swap loaders, vector stores, or LLMs easily
- ğŸ” Semantic search â€“ better accuracy for factual questions
- ğŸ’» Local execution â€“ no external API calls
- ğŸ› ï¸ Customizable â€“ extend with agents, tools, tracing

---

## âŒ Limitations

- ğŸ§  Local LLMs like llama2 may not match GPT-4 level reasoning
- ğŸ¢ Larger models require more RAM/VRAM
- ğŸ§ª Some components will need updating in future LangChain versions

---

## ğŸ™‹â€â™‚ï¸ Example Usage

```python
# Example question to the pipeline
Question: "What is the purpose of FAISS in the pipeline?"

# LLM Answer:
"FAISS is used to store and retrieve text chunks based on their semantic similarity. It enables fast vector search over embedded document chunks."
```

---

## ğŸ“¬ Contributions

PRs and suggestions welcome! Feel free to fork and submit a pull request.

---

## ğŸ“„ License

MIT License â€“ use freely and responsibly.

---

Let me know if you'd like badges, diagrams, or a version with advanced features (e.g., multiple retrievers, GUI integration).

